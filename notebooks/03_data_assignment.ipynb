{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a55dd82",
   "metadata": {},
   "source": [
    "(03-data)=\n",
    "<!-- # Day 2 (Take-Home) Programmatic Data Operations -->\n",
    "# Programmatic Data Operations\n",
    "\n",
    "*Author*: Zach del Rosario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899e467",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to give you some tools to work with data *programmatically*; that is, using a programming language. While you can carry out many data operations by hand or with spreadsheet programs, you will see that doing things programmatically is extremely powerful. \n",
    "\n",
    "### Learning Outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Use Pandas' `DataFrame` object to represent data\n",
    "- Use DataFrame operations from the package `py-grama` to operate on data\n",
    "- Use basic data checks: shapes, data types, head and tail\n",
    "- Perform fundamental data wrangling: type conversions, pivoting, filtering, mutating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import grama as gr\n",
    "%matplotlib inline\n",
    "\n",
    "DF = gr.Intention()\n",
    "\n",
    "# For downloading data\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69417f00",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d65551",
   "metadata": {},
   "source": [
    "A `DataFrame` is a data structure provided by Pandas. In contrast with `lists` (which we saw in a previous exercise), DataFrames are explicitly designed to facilitate data analysis. Accordingly, they provide a number of helpful features that aid in data analysis and operations.\n",
    "\n",
    "A `DataFrame` is a *rectangular* representation of data -- it consists of rows and columns. If the data are *tidy*, then each *row* represents an *observation*, each *column* represents a *variable*, and each *cell* represents a single measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf45c9",
   "metadata": {},
   "source": [
    "For instance, the following code chunk downloads a alloy dataset into the DataFrame `df_mpea`---here each row is an alloy, and each column is some physical property of that alloy. These data come from the report [Borg et al.  2020](https://www.nature.com/articles/s41597-020-00768-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fe396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for local data\n",
    "filename_data = \"./data/mpea.csv\"\n",
    "\n",
    "# The following code downloads the data, or (after downloaded)\n",
    "# loads the data from a cached CSV on your machine\n",
    "if not os.path.exists(filename_data):\n",
    "    # Make request for data\n",
    "    url_data = \"https://docs.google.com/spreadsheets/u/1/d/1MsF4_jhWtEuZSvWfXLDHWEqLMScGCVXYWtqHW9Y7Yt0/export?format=csv\"\n",
    "    r = requests.get(url_data, allow_redirects=True)\n",
    "    open(filename_data, 'wb').write(r.content)\n",
    "    print(\"   MPEA data downloaded from public Google sheet\")\n",
    "else:\n",
    "    # Note data already exists\n",
    "    print(\"    MPEA data loaded locally\")\n",
    "    \n",
    "# Read the data into memory\n",
    "df_mpea = pd.read_csv(filename_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc97ada",
   "metadata": {},
   "source": [
    "Let's use some of the basic attributes of the data to get some basic facts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1816fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df_mpea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877b2f4",
   "metadata": {},
   "source": [
    "We have `1653` rows (also called observations) on 20 columns (also called variables or features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpea.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce11208",
   "metadata": {},
   "source": [
    "The `head` method shows just the top of the DataFrame; this is useful for \"getting a sense\" for what's in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpea.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c16daf",
   "metadata": {},
   "source": [
    "The `dtypes` attribute gives us the data type for each column. Depending on the dataset, you might find that your data loads in with strange datatypes. This can happen, for instance, if your numeric values are contained within string characters (e.g. `\"1.23\"`). If this happens, you can catch the fact with a call to `df_data.dtypes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd2415",
   "metadata": {},
   "source": [
    "### __Q1__: Inspect a DataFrame\n",
    "\n",
    "Consult the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) (it might be useful to use a page search) and use some basic calls on `df_mpea` to answer the following questions:\n",
    "\n",
    "- What are the *last* five observations in the DataFrame?\n",
    "- How many rows are in `df_mpea`? How many columns?\n",
    "- How would you access the column `PROPERTY: Microstructure`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5009658",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_data\n",
    "# TODO: Show the last five observations of df_mpea\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_mpea\n",
    "# TODO: Determine the number of rows and columns in df_mpea\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_data\n",
    "# TODO: Grab the column `PROPERTY: Microstructure` alone\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f1fcd",
   "metadata": {},
   "source": [
    "These manipulations are simple, but they are bread-and-butter for studying new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9929942",
   "metadata": {},
   "source": [
    "## Grama\n",
    "\n",
    "---\n",
    "\n",
    "The `py-grama` package builds on top of Pandas to provide a pipeline-based data (and model) infrastructure. Grama provides many of the same functions as Pandas (really, just different ways to use the same Pandas functions):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f60afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "   df_mpea\n",
    "   >> gr.tf_head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560489e5",
   "metadata": {},
   "source": [
    "One of the advantages of using `py-grama` is that we can write *data pipelines* to organize our data operations. For instance, the following code filters the MPEA dataset to only those cases that have a valid Yield Strength (YS) and Ultimate Tensile Strength (UTS), and computes a correlation coefficient between those two quantities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cd2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and see the result\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF[\"PROPERTY: YS (MPa)\"]),\n",
    "        gr.not_nan(DF[\"PROPERTY: UTS (MPa)\"]),\n",
    "    )\n",
    "    >> gr.tf_summarize(\n",
    "        rho_YS_UTS=gr.corr(\n",
    "            DF[\"PROPERTY: YS (MPa)\"],\n",
    "            DF[\"PROPERTY: UTS (MPa)\"],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376fcc8",
   "metadata": {},
   "source": [
    "As we might expect, these two properties are strongly correlated.\n",
    "\n",
    "This code shows off a few concepts, which we'll explore below: The pipe operator `>>`, Grama verbs (such as `tf_filter`), and the data pronoun `DF`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852353d",
   "metadata": {},
   "source": [
    "### The pipe operator `>>`\n",
    "\n",
    "It's helpful to think of the pipe operator `>>` as the words \"and then\". That means code like this:\n",
    "\n",
    "```\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter( ... )\n",
    "    >> gr.tf_mutate( ... )\n",
    "    >> gr.tf_pivot_longer( ... )\n",
    ")\n",
    "```\n",
    "\n",
    "Can be read something like an English sentence, where we are using various *verbs* to operate on the data:\n",
    "\n",
    "```\n",
    "(\n",
    "    Start with df_mpea\n",
    "    and then filter the data\n",
    "    and then mutate the data\n",
    "    and then pivot the data in to a longer format\n",
    ")\n",
    "```\n",
    "\n",
    "We don't yet know what these verbs do; we'll learn more in the exercises below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e4c40",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "The `tf_select` verb allows us to select one or more columns; this is helpful when we want to focus on just a handful of properties, such as the chemical formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(\"FORMULA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a3114",
   "metadata": {},
   "source": [
    "We can select multiple columns by providing multiple arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13cb57f",
   "metadata": {},
   "source": [
    "### __Q2__: Use `tf_select` to select the formula and microstructure columns only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Select the formula and microstructure columns only\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf356e99",
   "metadata": {},
   "source": [
    "We can also use some *selection helpers* to make `tf_select` even more convenient. For instance, the `gr.everything()` function just selects all the columns, which at first seems silly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd5602",
   "metadata": {},
   "source": [
    "However, when we use `gr.everything()` *along* with specific columns, we can *re-arrange* the columns to make quick comparisons easier. For instance, let's move the reference information to the left. We could then easily copy the DOI's to find the original reference for each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25491bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(\"REFERENCE: doi\", gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e6f80",
   "metadata": {},
   "source": [
    "There are a variety of other selection helpers, including:\n",
    "\n",
    "- `gr.starts_with(...)` will select all columns that start with a given string\n",
    "- `gr.ends_with(...)` will select all columns that end with a given string\n",
    "- `gr.contains(...)` will select all columns that contain a given string\n",
    "- `gr.matches(...)` will select all columns that match a given [regular expression](https://regexone.com/)\n",
    "\n",
    "You'll practice using selection helpers in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0f68eb",
   "metadata": {},
   "source": [
    "### __Q3__ Use a selection helper to find **all** of the columns with the string `\"REFERENCE\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Select the formula and microstructure columns only\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5068c",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "\n",
    "Aside from selecting columns, we can also make convenience modifications to the data. The verb `tf_rename` allows us to rename columns, usually to create a more compact, convenient name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(\n",
    "        microstructure=\"PROPERTY: Microstructure\",\n",
    "    )\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256c536",
   "metadata": {},
   "source": [
    "This is a good time to step aside from verbs to talk about the *data pronoun*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231a98a",
   "metadata": {},
   "source": [
    "## Interlude: Pipelines and the \"Data Pronoun\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0f5b0",
   "metadata": {},
   "source": [
    "Imagine we wanted to search through the dataset to find only those materials with a FCC microstructure. Above, we gave the `microstructure` column a new, convenient name. We might like to use that new, convenient name when searching for FCC materials. However, we're going to run into an issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Try uncommenting and running the following code; it WILL break!\n",
    "# (\n",
    "#     df_mpea\n",
    "#     >> gr.tf_rename(\n",
    "#         microstructure=\"PROPERTY: Microstructure\",\n",
    "#     )\n",
    "#     >> gr.tf_filter(\n",
    "#         df_mpea[\"microstructure\"] == \"FCC\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9419a77",
   "metadata": {},
   "source": [
    "If we want to refer to the data *now*---as it is currently in the pipeline---we need a name to refer to that DataFrame. This is where the *data pronoun* comes in; remember when we ran this line way up above in the setup chunk?\n",
    "\n",
    "```\n",
    "DF = gr.Intention()\n",
    "```\n",
    "\n",
    "This assigns the data pronoun to the name `DF`. The data pronoun represents a DataFrame, so we can use things like column access `DF[\"column name\"]`. We can use this to take advantage of the new (shorter) name we gave to the microstructure column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5670d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(\n",
    "        microstructure=\"PROPERTY: Microstructure\",\n",
    "    )\n",
    "    >> gr.tf_filter(\n",
    "        DF[\"microstructure\"] == \"FCC\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafdceb",
   "metadata": {},
   "source": [
    "Together, the pipe operator `>>` and the data pronoun `DF` form a powerful team that helps us do sophisticated data operations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39cbee",
   "metadata": {},
   "source": [
    "### __Q4__ Re-write the following code using the pipe operator and data pronoun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Eliminate the intermediate variables by using the data pronoun\n",
    "###\n",
    "\n",
    "# -- NO NEED TO EDIT; REWRITE THIS CODE -----\n",
    "# Set up a simple dataset\n",
    "df_initial = gr.df_make(\n",
    "    A=[1, 2, 3],\n",
    "    longcolumnname=[4, 5, 6],\n",
    ")\n",
    "print(df_initial)\n",
    "\n",
    "df_new = (\n",
    "    df_initial\n",
    "    >> gr.tf_rename(B=\"longcolumnname\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_new\n",
    "    >> gr.tf_filter(df_new.B == 5)\n",
    ")\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "(\n",
    "    df_initial\n",
    "    # Use pipes >> and the data pronoun DF;\n",
    "    # you should only need two lines of code\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d3085",
   "metadata": {},
   "source": [
    "## Back to Verbs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e428b",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "We saw `tf_filter` above; this allows us to filter a dataset to only those rows satisfying some logical criterion. This makes answering basic questions about the data very easy. For instance, we might be interested in a particular processing method; we could find only those rows matching a specified method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(DF[\"PROPERTY: Processing method\"] == \"POWDER\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2bc08",
   "metadata": {},
   "source": [
    "Notice that not all of the cells have useful values; some have `NaN` as their value (which means Not a Number). These could be due to any of a number of potential issues; perhaps the original reference did not report that value, meaning that information exists but is missing.\n",
    "\n",
    "There are some helper functions to help deal with `NaN` values in filters: `gr.not_nan(df.column)` will return `True` when its input is not `NaN`, while `gr.is_nan(df.column)` will do the reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc1d9f",
   "metadata": {},
   "source": [
    "### __Q5__ Filter the MPEA dataset to only those rows with a valid Yield Strength. Compare the original number of rows with the number of valid rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b409c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Filter the data to find the non-NaN Yield Strength values\n",
    "###\n",
    "\n",
    "# -- NO NEED TO EDIT; USE FOR COMPARISON -----\n",
    "print(\"Original shape: {}\".format(df_mpea.shape))\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedcdeb",
   "metadata": {},
   "source": [
    "### Mutating\n",
    "\n",
    "The `tf_mutate` verb allows us to create / modify columns based on existing column values. For instance, we could use a mutation to convert the units in a column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_mutate(\n",
    "        E_MPa = DF[\"PROPERTY: Young modulus (GPa)\"] * 1000\n",
    "    )\n",
    "    >> gr.tf_select(\"E_MPa\", gr.everything())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bb9e9",
   "metadata": {},
   "source": [
    "This might be useful if we aimed to compare two quantities; elasticity and ultimate tensile strength are somewhat related properties, so we might want to compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393500e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_mutate(\n",
    "        E_MPa = DF[\"PROPERTY: Young modulus (GPa)\"] * 1000\n",
    "    )\n",
    "    >> gr.tf_rename(\n",
    "        UTS_MPa = \"PROPERTY: UTS (MPa)\"\n",
    "    )\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF.UTS_MPa),\n",
    "        gr.not_nan(DF.E_MPa),\n",
    "    )\n",
    "    >> gr.tf_select(\n",
    "        \"UTS_MPa\",\n",
    "        \"E_MPa\",\n",
    "        gr.everything(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef91c5",
   "metadata": {},
   "source": [
    "### __Q6__ Convert the weight parts per million (wppm) of Oxygen to a (weight) percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Convert wppm to a weight percentage\n",
    "# NOTE: There is some scaffolding code; you need only\n",
    "#       write the call to tf_mutate\n",
    "###\n",
    "\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF[\"PROPERTY: O content (wppm)\"]),\n",
    "    )\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "    ## TODO: Use gr.tf_mutate to do the conversion\n",
    "\n",
    "    >> gr.tf_select(\"O_percent\", gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f86610",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "\n",
    "---\n",
    "\n",
    "With these basic tools---data pipelines and verbs---we have many of the tools we need to do *data wrangling*. *Very* frequently, data are messy and unusable; we need to do some *wrangling* to get our data into-shape for analysis. The last few tasks will focus on key steps in data wrangling.\n",
    "\n",
    "### Data Converting\n",
    "\n",
    "There's something wrong with the Nitrogen content column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(gr.contains(\"content\"))\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686f03e",
   "metadata": {},
   "source": [
    "The Oxygen and Carbon content columns are fine---they're `float64`, which is a numeric type as we'd expect. But the Nitrogen content is an `object`. Let's see what specific values this column takes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3501ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(\n",
    "    df_mpea[\"PROPERTY: N content (wppm)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef20b7",
   "metadata": {},
   "source": [
    "```{admonition} Use of set\n",
    "The `set` datatype only allows one of each unique value; calling `set()` on a column is a simple way to find all the unique values in a column.\n",
    "```\n",
    "\n",
    "It seems that the original data are mixed; some values are a numeric ppm value, while others are the qualitative statement `\"undetectable\"` (and yet others are simply missing). We can use the Grama helper `gr.as_numeric()` to help convert the data to a numeric type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2def780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(N_wppm=\"PROPERTY: N content (wppm)\")\n",
    "    >> gr.tf_mutate(N_converted=gr.as_numeric(DF.N_wppm))\n",
    "    >> gr.tf_select(\"N_wppm\", \"N_converted\")\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0d680",
   "metadata": {},
   "source": [
    "With this type conversion, we can express all three element columns as percentages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9641a4",
   "metadata": {},
   "source": [
    "### __Q7__ Fix the conversion from wppm to a weight percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fix the conversion of N wppm\n",
    "# NOTE: There is some scaffolding code; you need only\n",
    "#       edit one line\n",
    "###\n",
    "\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_mutate(\n",
    "        O_percent=DF[\"PROPERTY: O content (wppm)\"] * 100 / 1e6,\n",
    "        C_percent=DF[\"PROPERTY: C content (wppm)\"] * 100 / 1e6,\n",
    "\n",
    "## --- WRITE YOUR CODE HERE -----\n",
    "        ## TODO: Fix the following line of code\n",
    "#         N_percent=DF[\"PROPERTY: N content (wppm)\"] * 100 / 1e6\n",
    "    )\n",
    "    >> gr.tf_select(gr.contains(\"percent\"), gr.everything())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e61c49",
   "metadata": {},
   "source": [
    "### Pivoting Data\n",
    "\n",
    "Another common data issue is when our data are in the wrong *shape*. To illustrate, let's look at another dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.data import df_stang_wide\n",
    "df_stang_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33aa6f",
   "metadata": {},
   "source": [
    "These are observations on different samples of the same rolled aluminum alloy, with measurements taken at different angles relative to the direction of rolling. Note that the relative angle `0, 45, 90` is in the column names, rather than in cells. This means we would need to write special-purpose code in order to analyze these data.\n",
    "\n",
    "Rather than re-invent our analysis code for every new dataset, we can instead *reshape* our data into a single, consistent format for a single set of analysis tools. To that end, we are going to reshape the data into a [tidy](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) format. \n",
    "\n",
    "Our goal will be to wrangle this messy, wide dataset into tidy, long format, shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73807c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.data import df_stang\n",
    "df_stang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbf53e",
   "metadata": {},
   "source": [
    "To carry out this reshaping, we will use a set of *pivoting* tools. As a simple example, `gr.tf_pivot_longer()` takes a wide dataset and makes it longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = (\n",
    "    gr.df_make(\n",
    "        A=[1, 2, 3],\n",
    "        B=[4, 5, 6],\n",
    "        C=[7, 8, 9],\n",
    "    )\n",
    ")\n",
    "print(df_tmp)\n",
    "\n",
    "(\n",
    "    df_tmp\n",
    "    >> gr.tf_pivot_longer(\n",
    "        columns=[\"A\", \"B\", \"C\"],\n",
    "        names_to=\"name\",\n",
    "        values_to=\"value\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca6240",
   "metadata": {},
   "source": [
    "### __Q8__ Pivot `df_stang_wide` longer to put all the angle values in cells\n",
    "\n",
    "*Hint 1*: Make sure to add an `observation` column with the `index_to` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44024ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Pivot the data longer\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "\n",
    "df_q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46aad5",
   "metadata": {},
   "source": [
    "Execute the following to check your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_q8.shape[0] == 54)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not sufficiently long; did you pivot?\")\n",
    "    \n",
    "try:\n",
    "    assert(df_q8.shape[1] == 5)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have five columns\")\n",
    "    \n",
    "try:\n",
    "    assert(\"observation\" in df_q8.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have have an `'observation'` column\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe9874",
   "metadata": {},
   "source": [
    "Next, we need to separate the measurement identifiers `\"E\", \"mu\"` from the angle measurements. For that, we can use the `gr.tf_separate()` verb. This allows us to take string values and split them into separate columns, based on a separator character:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gr.df_make(\n",
    "        combined=[\"a-1\", \"b-2\", \"c-3\"],\n",
    "    )\n",
    "    >> gr.tf_separate(\n",
    "        column=\"combined\",\n",
    "        into=[\"letter\", \"number\"],\n",
    "        sep=\"-\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6f9d5",
   "metadata": {},
   "source": [
    "### __Q9__ Use `gr.tf_separate()` to separate the measurement identifiers `\"E\", \"mu\"` from the measurement angles. Make sure to call the angle column `\"angle\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42072a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Pivot the data longer\n",
    "###\n",
    "\n",
    "df_q9 = (\n",
    "    df_q8\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "    ## Use gr.tf_separate()\n",
    "\n",
    ")\n",
    "df_q9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1c60d",
   "metadata": {},
   "source": [
    "Use the following code to check your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_q9.shape[0] == df_q8.shape[0])\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not the right length; how did that happen?\")\n",
    "    \n",
    "try:\n",
    "    assert(df_q9.shape[1] == 6)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have six columns\")\n",
    "    \n",
    "try:\n",
    "    assert(\"angle\" in df_q9.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'angle' column\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a02c65",
   "metadata": {},
   "source": [
    "We're nearly there! Finally, we need to turn the measurement identifiers `\"E\", \"mu\"` back into column names. We can do this by pivoting wider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa377c52",
   "metadata": {},
   "source": [
    "### __Q10__ Pivot the data wider to turn the measurement identifiers `\"E\", \"mu\"` back into column names.\n",
    "\n",
    "*Hint*: You should only need to set the `names_from` and `values_from` arguments with this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Pivot the data wider\n",
    "###\n",
    "\n",
    "df_q10 = (\n",
    "    df_q9\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "    ## Use gr.tf_pivot_wider()\n",
    "\n",
    ")\n",
    "df_q10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15967b74",
   "metadata": {},
   "source": [
    "Use the following code to check your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_q10.shape[0] == 27)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not the right length; how did that happen?\")\n",
    "    \n",
    "try:\n",
    "    assert(\"angle\" in df_q10.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'angle' column\")\n",
    "    \n",
    "try:\n",
    "    assert(\"E\" in df_q10.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'E' column\")\n",
    "    \n",
    "try:\n",
    "    assert(\"mu\" in df_q10.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'mu' column\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd434c",
   "metadata": {},
   "source": [
    "### Bonus: One-step pivot\n",
    "\n",
    "As a closing, bonus demonstration, we illustrate some advanced options in `gr.tf_pivot_longer()` that would allow you to reshape the data in single call. This uses the `\".value\"` special argument to signal that the values that would be placed in that `names_to` column should really be column names themselves; put differently, the `\".value\"` keyword is like adding a pivot wider after the pivot longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2284e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_stang_wide\n",
    "    >> gr.tf_pivot_longer(\n",
    "        columns=[\"E_00\", \"mu_00\", \"E_45\", \"mu_45\", \"E_90\", \"mu_90\"],\n",
    "        names_to=[\".value\", \"angle\"],\n",
    "        names_sep=\"_\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75641500",
   "metadata": {},
   "source": [
    "# Survey\n",
    "\n",
    "---\n",
    "\n",
    "Once you complete this activity, please fill out the following 30-second survey:\n",
    "\n",
    "> [Survey link](https://forms.gle/wyZCz4MbquZArzTXA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391316a2",
   "metadata": {},
   "source": [
    "## Endnotes\n",
    "\n",
    "- The data portions of Grama make heavy use of ideas from the [Tidyverse](https://www.tidyverse.org/); specifically the [dplyr](https://dplyr.tidyverse.org/) package. However, those packages are for the R programming language.\n",
    "- The HEA dataset is from:\n",
    "\n",
    "> Borg, C.K.H., Frey, C., Moh, J. et al. Expanded dataset of mechanical properties and observed phases of multi-principal element alloys. Sci Data 7, 430 (2020). https://doi.org/10.1038/s41597-020-00768-9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
