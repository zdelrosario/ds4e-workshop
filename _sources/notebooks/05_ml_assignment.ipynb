{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28fcbce7",
   "metadata": {},
   "source": [
    "## Exercises in Machine Learning\n",
    "\n",
    "*Author*: Zach del Rosario\n",
    "\n",
    "The _primary_ purpose of this notebook is to help you *not get fooled by machine learning*! As Drew Conway notes, possessing hacking skills and substantive experience -- but having no math or statistics background -- puts one in the [danger zone](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram). While we can't possibly cover *everything* you need in a single workshop, this exercise will highlight some of the challenges of doing machine learning well. \n",
    "\n",
    "### Learning outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "* Use scikit-learn to fit regression models to data\n",
    "* Use cross-validation to help avoid *underfitting* and *overfitting* of data\n",
    "\n",
    "Tips:\n",
    "\n",
    "* This exercise will make heavy use of [scikit-learn](https://scikit-learn.org/stable/); you can find lots of useful info on the [documentation site](https://scikit-learn.org/stable/documentation.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as pt\n",
    "import grama as gr\n",
    "\n",
    "DF = gr.Intention()\n",
    "\n",
    "# Model training tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bea036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_noise(function, sigma=0.1, seed=101):\n",
    "    \"\"\"Add noise to deterministic functions\n",
    "    \"\"\"\n",
    "    def new_function(x):\n",
    "        np.random.seed(seed)\n",
    "        y = function(x)\n",
    "        return y + sigma * np.random.normal(size=y.shape)\n",
    "\n",
    "    return new_function\n",
    "\n",
    "# Reference points for regression examples\n",
    "X_ref = np.linspace(-1, +1, num=100)\n",
    "\n",
    "# Reference models\n",
    "def fcn_1(x): return (0.3 * x**2 + 1.0 * x + 2)\n",
    "def fcn_2(x): return (-2.0 * x**3 + 0.4 * x**2 + 1.0 * x + 2)\n",
    "\n",
    "fcn_1_noisy = gr.make_symbolic(add_noise(fcn_1))\n",
    "fcn_2_noisy = gr.make_symbolic(add_noise(fcn_2, sigma=0.2))\n",
    "\n",
    "# Package as a dataframe\n",
    "df_data = gr.df_make(\n",
    "    x=X_ref,\n",
    "    y_1=fcn_1_noisy(X_ref),\n",
    "    y_2=fcn_2_noisy(X_ref),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6439a6",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "---\n",
    "\n",
    "(This function will automate some steps so we can focus on high-level ideas, but I define it here in case you'd like to see the details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For partial evaluation\n",
    "from toolz import curry\n",
    "\n",
    "## Encapsulate featurizer and regressor in one function\n",
    "class FunctionLM(gr.Function):\n",
    "    def __init__(self, regressor, var, out, name, order, runtime):\n",
    "        self.regressor = regressor\n",
    "        self.var = var\n",
    "        self.out = list(map(lambda s: s + \"_mean\", out))\n",
    "        self.name = name\n",
    "        self.order = order\n",
    "        self.runtime = runtime\n",
    "\n",
    "    def eval(self, df):\n",
    "        ## Check invariant; model inputs must be subset of df columns\n",
    "        if not set(self.var).issubset(set(df.columns)):\n",
    "            raise ValueError(\n",
    "                \"Model function `{}` var not a subset of given columns\".format(\n",
    "                    self.name\n",
    "                )\n",
    "            )\n",
    "\n",
    "        ## Featurize\n",
    "        x = np.atleast_2d(df[self.var].values)\n",
    "        poly = PolynomialFeatures(self.order)\n",
    "        X_feat = poly.fit_transform(x)\n",
    "        \n",
    "        ## Predict\n",
    "        y = self.regressor.predict(X_feat).flatten()\n",
    "        return pd.DataFrame(data=y, columns=self.out)\n",
    "\n",
    "## Fitting routine\n",
    "@curry\n",
    "def fit_regression(df, var=None, out=None, order=1):\n",
    "    r\"\"\"Fit a linear regression of specified order\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Data for fitting\n",
    "        var (iterable of str): Names of input variable (feature); must be column in df\n",
    "        out (iterable of str): Name of output variable (response); must be column in df\n",
    "        order (int): Polynomial order for fit\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(out) > 1:\n",
    "        raise ValueError(\"This simple helper\")\n",
    "    # Featurize\n",
    "    x = np.atleast_2d(df[var].values)\n",
    "    poly = PolynomialFeatures(order)\n",
    "    X_feat = poly.fit_transform(x)\n",
    "    \n",
    "    # Fit regression\n",
    "    y = np.atleast_2d(df[out].values)\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_feat, y)\n",
    "    \n",
    "    # Package\n",
    "    fun = FunctionLM(lm, var, out, \"Linear Model\", order, 0)\n",
    "    \n",
    "    return gr.Model(functions=[fun], domain=None, density=None)\n",
    "    \n",
    "## Create pipe-enabled regression utility\n",
    "ft_regression = gr.add_pipe(fit_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca3936",
   "metadata": {},
   "source": [
    "## Primer: The Key Ideas\n",
    "First we'll cover some key ideas on simple functions. These are not 'real' data, but the simplicity of the examples will allow us to focus on concepts.\n",
    "\n",
    "Here I generate some data from a simple polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcee289",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Ground-truth data; no noise\n",
    "df_ex1 = (\n",
    "    gr.df_make(x=np.linspace(-1, +1, num=10))\n",
    "    >> gr.tf_mutate(y=fcn_1(DF.x))\n",
    ")\n",
    "\n",
    "# Plot\n",
    "(\n",
    "    df_ex1\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y\"))\n",
    "    + pt.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c98ec2",
   "metadata": {},
   "source": [
    "We will fit a simple [linear regression](https://en.wikipedia.org/wiki/Linear_regression) to these data. To do this, we'll use the implementation `LinearRegression()` from scikit-learn. To start, we'll assume that the data were generated from an underlying rule (a _model_) of the form\n",
    "\n",
    "$$y = m x + b,$$\n",
    "\n",
    "and attempt to _learn_ the slope $m$ and intercept $b$ by _minimizing_ the difference between the measured values `Y_1` and the predicted values `Y_1_linear_pred`.\n",
    "\n",
    "### Q1: Fit a linear regression\n",
    "Use the scikit-learn function `LinearRegression()` to fit a line to the data `X_c, Y_1`.\n",
    "\n",
    "Note: You will have to [look up](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) the documentation for `LinearRegression()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f02be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fit a line on X_c, Y_1\n",
    "# TODO: Fit a scikit learn regression with LinearRegression()\n",
    "###\n",
    "\n",
    "reg_1_linear = None\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "\n",
    "\n",
    "# Predict using fitted model\n",
    "Y_1_ref = fcn_1(X_ref)\n",
    "Y_1_linear_pred = reg_1_linear.predict(\n",
    "    np.atleast_2d(X_ref))  # Predict using model from above\n",
    "\n",
    "# Compute mean-squared error\n",
    "nde_1 = nde(Y_1_ref, Y_1_linear_pred)\n",
    "print(\"nde = {0:4.3f}\".format(nde_1))\n",
    "print(\"(Should be nde = 0.158)\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(X_ref, Y_1_ref, 'k--', label=\"Truth\")\n",
    "plt.plot(X_c, Y_1, 'k.', label=\"Measured\")\n",
    "plt.plot(X_ref, Y_1_linear_pred, 'b-', label=\"Model\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b06dc",
   "metadata": {},
   "source": [
    "### Model Flexibility and _Underfitting_\n",
    "Note that we had to _assume_ a model-form in order to do the fitting. From the figure above, we can see that the model is close to the true values, but obviously lacks the curvature of the true data-generating process.\n",
    "\n",
    "This phenomenon -- failing to capture behavior in the data -- is called _underfitting_. This leads to error in the model, which is quantified above using _non-dimensional error_ (NDE). To reduce this contribution to error, we need to make our model _more flexible_ -- one way to do this is to add additional _features_ for the model to fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c28b5",
   "metadata": {},
   "source": [
    "### Featurization\n",
    "In this simple problem, we only have a single input $x$. However, we can generate additional _features_ on which to fit by considering additional _powers_ of $x$. For instance, we could fit a quadratic model\n",
    "\n",
    "$$y = b x^0 + m_1 x^1 + m_2 x^2.$$\n",
    "\n",
    "It is a _common misconception_ that linear regression _cannot fit nonlinear models_. Clearly the model above is nonlinear, but it is _linear in the features_ $[1, x^1, x^2]$. By providing the additional quadratic term, we give the model more flexibility to fit patterns in the data. Below, you will compute this quadratic featurization by using the `PolynomialFeatures()` function from scikit-learn.\n",
    "\n",
    "### Aside: Features as data\n",
    "It may seem strange that we are featurizing by modifying the data -- we're essentially adding \"fake\" variables to our dataset. So long as we keep track of which variables are real and which are synthetic, this will not be an issue. In the next workshop exercise (on Sequential Learning), we will see ways to bring in additional \"real\" variables based on physical featurizations -- in that case, we will also be adding columns.\n",
    "\n",
    "### Q2: Build polynomial features\n",
    "Use the scikit-learn function `PolynomialFeatures()` to build a matrix of values $[1, x^1, x^2]$.\n",
    "\n",
    "Note: You'll probably need to look up the documentation for `PolynomialFeatures()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76256431",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Featurize the data\n",
    "# TODO: Do a Google search for \"scikit-learn PolynomialFeatures\" to find the documentation\n",
    "# TODO: Generate a transformation object using PolynomialFeatures(), assign to poly_2d\n",
    "# TODO: Compute a featurization of the data to add quadratic term, assign to X_quad\n",
    "# Use \"a method\" of poly_2d on X_c\n",
    "###\n",
    "\n",
    "# -- UNCOMMENT AND FINISH THE CODE BELOW -----\n",
    "# poly_2d = None\n",
    "# X_quad = None\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "\n",
    "print(X_quad)\n",
    "print(\"(Should be)\\n [Constant,    Linear,     Quadratic]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08bf31f",
   "metadata": {},
   "source": [
    "Note that this featurization gives rows of $[x^0, x^1, x^2]$, as we discussed above. Now we can use the featurized data to fit a quadratic model, and can use the same transform to evaluate the model on the reference points `X_ref`.\n",
    "\n",
    "Once you have generated `X_quad`, we can visualize the features to make sure they're what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(X_c, X_quad[:, 0], label=\"Constant\")\n",
    "plt.plot(X_c, X_quad[:, 1], label=\"Linear\")\n",
    "plt.plot(X_c, X_quad[:, 2], label=\"Quadratic\")\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d8054",
   "metadata": {},
   "source": [
    "By passing these features to `LinearRegression()`\n",
    "\n",
    "### Q3: Fit a quadratic regression\n",
    "Use the transform `poly_2d` and featurized data `X_quad` to fit and evaluate a quadratic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Use your featurization to fit a new model\n",
    "# TODO: Perform regression on X_quad, assign to reg_1_quad\n",
    "# TODO: Compute predictions on X_ref, assign to Y_1_quad_pred\n",
    "# make sure to featurize the inputs!\n",
    "###\n",
    "\n",
    "# -- UNCOMMENT AND FINISH THIS CODE -----\n",
    "# reg_1_quad = None\n",
    "# Y_1_quad_pred = None\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "\n",
    "# Compute mean-squared error\n",
    "nde_1_quad = nde(Y_1_ref, Y_1_quad_pred)\n",
    "print(\"nde = {0:4.3f}\".format(nde_1_quad))\n",
    "print(\"(Should be nde = 0.000)\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(X_ref, Y_1_ref, 'k--', label=\"Truth\")\n",
    "plt.plot(X_c, Y_1, 'k.', label=\"Measured\")\n",
    "plt.plot(X_ref, Y_1_quad_pred, 'b-', label=\"Model\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151cff0",
   "metadata": {},
   "source": [
    "Here we can see the model fits the data _perfectly_, which is corroborated by `nde = 0.000`. This _suggests_ that we have successfully discovered the _exact_ rule that generated these data, which in _this special case happens to be true_.\n",
    "\n",
    "However, we will very rarely be able to fit the true function exactly. This is because real data tend to have _noise_, which corrupts the underlying function we are trying to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf3a84",
   "metadata": {},
   "source": [
    "### Noise and _Overfitting_\n",
    "Below, I generate data from the same model, but add a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy data\n",
    "Y_1_noisy = fcn_1_noisy(X_c)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_c, Y_1_noisy, \"b.\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y + noise\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef6410",
   "metadata": {},
   "source": [
    "First, try fitting a quadratic to these new data, and inspect the fit.\n",
    "\n",
    "### Q4: Fit a quadratic model to noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc10998",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fit a model on X_1, Y_1_noisy\n",
    "# TODO: Compute predictions, assign to Y_1_noisy_pred\n",
    "###\n",
    "\n",
    "# -- UNCOMMENT AND FINISH THIS CODE -----\n",
    "# Y_1_noisy_pred = None\n",
    "\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "# Compute the non-dimensional error\n",
    "nde_1_noisy = nde(Y_1_ref, Y_1_noisy_pred)\n",
    "print(\"nde = {0:4.3f}\".format(nde_1_noisy))\n",
    "print(\"(Should be nde = 0.139)\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure()\n",
    "plt.plot(X_ref, Y_1_ref, 'k--', label=\"True\")\n",
    "plt.plot(X_c, Y_1_noisy, 'k.', label=\"Measured\")\n",
    "plt.plot(X_ref, Y_1_noisy_pred, 'b-', label=\"Model\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949d4e1",
   "metadata": {},
   "source": [
    "Here we can see that the fit is no longer perfect, despite coming from the \"same\" model. This is also reflected in the finite NDE value. Since we already know that a quadratic can fit the underlying function perfectly, underfitting is not the issue here. Instead, the error is increased due to the noise in the data.\n",
    "\n",
    "_However_, we have not yet seen a case of _overfitting_. To see that phenomenon, let's consider a slightly more complicated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from the true function\n",
    "Y_2_ref = fcn_2(X_ref)\n",
    "# Generate noisy data\n",
    "Y_2_noisy = fcn_2_noisy(X_c)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_ref, Y_2_ref, 'k--', label=\"True\")\n",
    "plt.plot(X_c, Y_2_noisy, 'k.', label=\"Measured\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037992fb",
   "metadata": {},
   "source": [
    "Here we can see a somewhat complicated function that is quite corrupted by noise. Below, I'm going to fit a number of polynomial models of different orders. In practice, we would like to _make a decision_ about what polynomial order to use. A sensible choice would be to pick the order that minimizes the error -- let's see which model accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DEMONSTRATION CODE, NO NEED TO EDIT -----\n",
    "\n",
    "# First, define a helper function to automate fitting a\n",
    "# regression of user-selected `order`\n",
    "\n",
    "\n",
    "def fit_poly(X, Y, order):\n",
    "    poly = PolynomialFeatures(order)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=False).fit(X_poly, Y)\n",
    "\n",
    "    return reg, poly\n",
    "\n",
    "\n",
    "# Setup to fit over different polynomial orders\n",
    "Ord_all = list(range(15))\n",
    "Ord_plot = [1, 3, 9]\n",
    "Y_pred_c = np.zeros((len(Ord_all), X_c.shape[0]))\n",
    "Y_pred_ref = np.zeros((len(Ord_all), X_ref.shape[0]))\n",
    "NDE_c = np.zeros(len(Ord_all))\n",
    "NDE_ref = np.zeros(len(Ord_all))\n",
    "\n",
    "# Loop over orders\n",
    "plt.figure()\n",
    "plt.plot(X_ref, Y_2_ref, 'k--')\n",
    "for i in range(len(Ord_all)):\n",
    "    # Fit model\n",
    "    reg, poly = fit_poly(X_c, Y_2_noisy, Ord_all[i])\n",
    "    # Predict on same data & on reference points\n",
    "    Y_pred_c[i] = reg.predict(poly.fit_transform(X_c))\n",
    "    Y_pred_ref[i] = reg.predict(poly.fit_transform(X_ref))\n",
    "    # Compute error *on same data* -> estimated error\n",
    "    NDE_c[i] = nde(Y_2_noisy, Y_pred_c[i])\n",
    "    # Compute error on reference points -> 'true' error\n",
    "    NDE_ref[i] = nde(Y_2_ref, Y_pred_ref[i])\n",
    "\n",
    "    # Plot curve\n",
    "    if Ord_all[i] in Ord_plot:\n",
    "        plt.plot(X_ref, Y_pred_ref[i], label=\"Order = {}\".format(Ord_all[i]))\n",
    "plt.plot(X_c, Y_2_noisy, 'k.')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544e5e5",
   "metadata": {},
   "source": [
    "Here I've selected just a few of the models to plot. We can see\n",
    "\n",
    "* The `Order = 1` case is underfit, like we saw in the example above\n",
    "* The `Order = 9` case curves tortuously to go through _every single point_; this is an example of _overfitting_\n",
    "* The `Order = 3` case is not perfect, but fairly close to the true (dashed) curve. This is a well-fit model.\n",
    "\n",
    "More generally, _overfitting_ is when the model fits to spurrious patterns in the data; essentially, we are fitting to noise, rather than signal. We would like to detect and avoid overfitting in practice! While we can see above some suspicious behavior based on the fitted curves, we might like a _quantitative_ way to compare models. We can do this with the NDE values, but there is a _subtle issue_ at play here.\n",
    "\n",
    "Let's compare the `NDE` values `Estimated` on only the available (noisy) data, and the error computed using evaluations from the `True` (noiseless) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d101bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Ord_all, NDE_c, label='Estimated')\n",
    "plt.plot(Ord_all, NDE_ref, label='True')\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel('Poly Order')\n",
    "plt.ylabel('ND Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95058434",
   "metadata": {},
   "source": [
    "Here we can see the `Estimated` and `True` error values _greatly diverge_. This is _highly problematic_ for two interrelated reasons:\n",
    "\n",
    "1. In practice, we would only have access to the `Estimated` curve, as the `True` curve relies on data we do not have.\n",
    "2. If we were to make a decision about `Poly Order` based on the `Estimated` curve, we would choose a much higher order than what would minimize the NDE in the `True` case.\n",
    "\n",
    "The underlying reason for the poor error estimate here is that _we are using the same data to both train and test the model_. We can improve our estimates for the error through various techniques; below, we will use the technique of _cross-validation_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf59be3",
   "metadata": {},
   "source": [
    "### Avoiding Optimistic Estimates: Cross-Validation\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a technique for estimating the error in a way that avoids the \"optimism\" we saw above. For the variant _k-fold cross-validation_, we split all our data into _folds_, and use these to build _training_ and _test_ sets. Generally:\n",
    "\n",
    "* _Training_ data are used to fit a model\n",
    "* _Test_ data are used to evaluate a model\n",
    "\n",
    "![CV schematic](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "(Fabian Flock, via Wikimedia)\n",
    "\n",
    "In each of our $k$ iterations, we _do not allow_ the model to see a test fold (`Test data` above) during training, and fit the model only on the remaining data (`Training data` above). After training, we compute our chosen error metric on the test fold. Finally, we repeat this process on each of the $k$ chosen folds. This gives us a set of less optimistic estimates for the error, which we can summarize e.g. as a mean CV error.\n",
    "\n",
    "This procedure is implemented in the scikit-learn function `cross_validate()`. Use this routine to estimate the NDE in the case where the polynomial order is `9`. You will have to choose a number of folds to run; while `5` to `10` is common, since we have so few data, you will need to use a smaller number of folds.\n",
    "\n",
    "### Q5: K-fold Cross-Validation\n",
    "Perform k-fold cross validation on the order `9` polynomial model using the scikit-learn function `cross_validate()`. You will need to look up the documentation for `cross_validate()`, and pick the `cv` strategy to use the k-fold strategy with a reasonable value for $k$. Make sure to report both the `train_score` and `test_score` values. Compare the two sets of values.\n",
    "\n",
    "*Hints:*\n",
    "- You can pass `nde_score` to the keyword argument `scoring` to have `cross_validate()` compute the NDE\n",
    "- You can use the helper function `estimator, poly = fit_poly()` function above to help complete this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Use cross_validate() on the 9th order model to estimate the NDE\n",
    "# TODO: Compute the cross validation scores and assign to `scores`\n",
    "# Hint, you can pass the helper function `nde_score` (defined above) to cross_validate()\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW -----\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a623b97",
   "metadata": {},
   "source": [
    "The `train_score` values are quite optimistic, while the `test_score` values are *abysmal*.\n",
    "\n",
    "Below, I show results for performing k-fold cross validation across the same set of polynomial orders as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39686cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDACTED....\n",
    "# -- NO NEED TO WRITE CODE HERE -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6411924",
   "metadata": {},
   "source": [
    "<img src=\"../images/05_cv_order.png\">\n",
    "<!-- task-end -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bb546",
   "metadata": {},
   "source": [
    "The individual CV estimates are reported as dots, while their mean is given as a line. Here we can see that the NDE takes reasonable values for order at or below `3`. Beyond this point, the NDE explodes as models begin to overfit wildly. These cross-validated error metrics would be far more informative for making a decision about polynomial order.\n",
    "\n",
    "Here we have just one tunable knob (polynomial order) that defines our model. More generally, these kinds of user-selected quantities are called [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Cross-validation and related techniques are key to _tuning hyperparameters_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb4f84",
   "metadata": {},
   "source": [
    "### Using Models for Materials Informatics\n",
    "In the final part of the workshop, we will discuss how to _use_ these machine learning models to do useful work in materials science. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
