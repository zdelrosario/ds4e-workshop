{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1bf46f",
   "metadata": {},
   "source": [
    "## Programmatic Data Operations\n",
    "\n",
    "*Authors: Zach del Rosario*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16e9c5",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to give you some tools to work with data *programmatically*; that is, using a programming language. While you can carry out many data operations by hand or with spreadsheet programs, you will see that doing things programmatically is extremely powerful. \n",
    "\n",
    "### Learning Outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Use Pandas' `DataFrame` object to represent data\n",
    "- Use DataFrame operations from the package `py-grama` to operate on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6feb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import grama as gr\n",
    "\n",
    "DF = gr.Intention()\n",
    "\n",
    "# For downloading data\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccf079",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9f681",
   "metadata": {},
   "source": [
    "A `DataFrame` is a data structure provided by Pandas. In contrast with `lists` (which we saw in the previous exercise), DataFrames are explicitly designed to facilitate data analysis. Accordingly, they provide a number of helpful features that aid in data analysis and operations.\n",
    "\n",
    "A `DataFrame` is a *rectangular* representation of data -- it consists of rows and columns. Each *row* represents an *observation* -- a single instance of data. Each *column* represents a *variable* -- a particular attribute of the observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c496b77",
   "metadata": {},
   "source": [
    "For instance, the following code chunk downloads a alloy dataset into the DataFrame `df_mpea` -- here each row is an alloy, and each column is some physical property of that alloy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbe10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for local data\n",
    "filename_data = \"./data/mpea.csv\"\n",
    "\n",
    "# The following code downloads the data, or (after downloaded)\n",
    "# loads the data from a cached CSV on your machine\n",
    "if not os.path.exists(filename_data):\n",
    "    # Make request for data\n",
    "    url_data = \"https://docs.google.com/spreadsheets/u/1/d/1MsF4_jhWtEuZSvWfXLDHWEqLMScGCVXYWtqHW9Y7Yt0/export?format=csv\"\n",
    "    r = requests.get(url_data, allow_redirects=True)\n",
    "    open(filename_data, 'wb').write(r.content)\n",
    "    print(\"   MPEA data downloaded from public Google sheet\")\n",
    "else:\n",
    "    # Note data already exists\n",
    "    print(\"    MPEA data loaded locally\")\n",
    "    \n",
    "# Read the data into memory\n",
    "df_mpea = pd.read_csv(filename_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91198b27",
   "metadata": {},
   "source": [
    "Let's use some of the basic attributes of the data to get some basic facts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86943726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "df_mpea.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e04d9c",
   "metadata": {},
   "source": [
    "We have `1653` rows (also called observations) on 20 columns (also called variables or features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f111350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpea.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b36de",
   "metadata": {},
   "source": [
    "The `head` method shows just the top of the DataFrame; this is useful for \"getting a sense\" for what's in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0400fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mpea.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542b12b",
   "metadata": {},
   "source": [
    "The `dtypes` attribute gives us the data type for each column. Depending on the dataset, you might find that your data loads in with strange datatypes. This can happen, for instance, if your numeric values are contained within string characters (e.g. `\"1.23\"`). If this happens, you can catch the fact with a call to `df_data.dtypes`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd00575",
   "metadata": {},
   "source": [
    "### __Q1__: Inspecting a DataFrame\n",
    "Consult the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) (it might be useful to use a page search) and use some basic calls on `df_mpea` to answer the following questions:\n",
    "\n",
    "- What are the *last* five observations in the DataFrame?\n",
    "- How many rows are in `df_mpea`? How many columns?\n",
    "- How would you access the column `PROPERTY: Microstructure`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67037195",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_data\n",
    "# TODO: Show the last five observations of df_mpea\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_mpea\n",
    "# TODO: Determine the number of rows and columns in df_mpea\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Inspect df_data\n",
    "# TODO: Grab the column `PROPERTY: Microstructure` alone\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05c5e0",
   "metadata": {},
   "source": [
    "These manipulations are simple, but they are bread-and-butter for studying new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544ff07",
   "metadata": {},
   "source": [
    "## Grama\n",
    "\n",
    "---\n",
    "\n",
    "The `py-grama` package builds on top of Pandas to provide a pipeline-based data (and model) infrastructure. Grama provides many of the same functions as Pandas (really, just different ways to use the same Pandas functions):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f407089",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "   df_mpea\n",
    "   >> gr.tf_head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89308fb",
   "metadata": {},
   "source": [
    "One of the advantages of using `py-grama` is that we can write *data pipelines* to organize our data operations. For instance, the following code filters the MPEA dataset to only those cases that have a valid Yield Strength (YS) and Ultimate Tensile Strength (UTS), and computes a correlation coefficient between those two quantities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a524559",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and see the result\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF[\"PROPERTY: YS (MPa)\"]),\n",
    "        gr.not_nan(DF[\"PROPERTY: UTS (MPa)\"]),\n",
    "    )\n",
    "    >> gr.tf_summarize(\n",
    "        rho_YS_UTS=gr.corr(\n",
    "            DF[\"PROPERTY: YS (MPa)\"],\n",
    "            DF[\"PROPERTY: UTS (MPa)\"],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09d3d2",
   "metadata": {},
   "source": [
    "As we might expect, these two properties are strongly correlated.\n",
    "\n",
    "This code shows off a few concepts, which we'll explore below: The pipe operator `>>`, Grama verbs (such as `tf_filter`), and the data pronoun `DF`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7a701",
   "metadata": {},
   "source": [
    "### The pipe operator `>>`\n",
    "\n",
    "It's helpful to think of the pipe operator `>>` as the words \"and then\". That means code like this:\n",
    "\n",
    "```\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter( ... )\n",
    "    >> gr.tf_mutate( ... )\n",
    "    >> gr.tf_pivot_longer( ... )\n",
    ")\n",
    "```\n",
    "\n",
    "Can be read something like an English sentence, where we are using various *verbs* to operate on the data:\n",
    "\n",
    "```\n",
    "(\n",
    "    Start with df_mpea\n",
    "    and then filter the data\n",
    "    and then mutate the data\n",
    "    and then pivot the data in to a longer format\n",
    ")\n",
    "```\n",
    "\n",
    "We don't yet know what these verbs do; we'll learn more in the exercises below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b150301",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "The `tf_select` verb allows us to select one or more columns; this is helpful when we want to focus on just a handful of properties, such as the chemical formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c897c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(\"FORMULA\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7de9a5",
   "metadata": {},
   "source": [
    "### __qX__ Use `tf_select` to select the formula and microstructure columns only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Select the formula and microstructure columns only\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c63c0",
   "metadata": {},
   "source": [
    "We can also use some *selection helpers* to make `tf_select` even more convenient. For instance, the `gr.everything()` function just selects all the columns, which at first seems silly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e946604",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69c8e8",
   "metadata": {},
   "source": [
    "However, when we use `gr.everything()` *along* with specific columns, we can *re-arrange* the columns to make quick comparisons easier. For instance, let's move the reference information to the left. We could then easily copy the DOI's to find the original reference for each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4195d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(\"REFERENCE: doi\", gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65a366",
   "metadata": {},
   "source": [
    "There are a variety of other selection helpers, including:\n",
    "\n",
    "- `gr.starts_with(...)` will select all columns that start with a given string\n",
    "- `gr.ends_with(...)` will select all columns that end with a given string\n",
    "- `gr.contains(...)` will select all columns that contain a given string\n",
    "- `gr.matches(...)` will select all columns that match a given [regular expression](https://regexone.com/)\n",
    "\n",
    "You'll practice using selection helpers in the next task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1928c38b",
   "metadata": {},
   "source": [
    "### __qX__ Use a selection helper to find **all** of the columns with the string `\"REFERENCE\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Select the formula and microstructure columns only\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f91d9d",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "\n",
    "Aside from selecting columns, we can also make convenience modifications to the data. The verb `tf_rename` allows us to rename columns, usually to create a more compact, convenient name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(\n",
    "        microstructure=\"PROPERTY: Microstructure\",\n",
    "    )\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecc98a",
   "metadata": {},
   "source": [
    "This is a good time to step aside from verbs to talk about the *data pronoun*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422731d6",
   "metadata": {},
   "source": [
    "## Interlude: Pipelines and the \"Data Pronoun\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a508daf",
   "metadata": {},
   "source": [
    "(Illustrate the use of the data pronoun)\n",
    "\n",
    "Imagine we wanted to search through the dataset to find only those materials with a FCC microstructure. Above, we gave the `microstructure` column a new, convenient name. We might like to use that new, convenient name when searching for FCC materials. However, we're going to run into an issue:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Try uncommenting and running the following code; it WILL break!\n",
    "# (\n",
    "#     df_mpea\n",
    "#     >> gr.tf_rename(\n",
    "#         microstructure=\"PROPERTY: Microstructure\",\n",
    "#     )\n",
    "#     >> gr.tf_filter(\n",
    "#         df_mpea[\"microstructure\"] == \"FCC\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c947d9",
   "metadata": {},
   "source": [
    "If we want to refer to the data *now*---as it is currently in the pipeline---we need a name to refer to that DataFrame. This is where the *data pronoun* comes in; remember when we ran this line way up above in the setup chunk?\n",
    "\n",
    "```\n",
    "DF = gr.Intention()\n",
    "```\n",
    "\n",
    "This assigns the data pronoun to the name `DF`. The data pronoun represents a DataFrame, so we can use things like column access `DF[\"column name\"]`. We can use this to take advantage of the new (shorter) name we gave to the microstructure column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(\n",
    "        microstructure=\"PROPERTY: Microstructure\",\n",
    "    )\n",
    "    >> gr.tf_filter(\n",
    "        DF[\"microstructure\"] == \"FCC\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f7437",
   "metadata": {},
   "source": [
    "Together, the pipe operator `>>` and the data pronoun `DF` form a powerful team that helps us do sophisticated data operations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ac95a",
   "metadata": {},
   "source": [
    "### __qX__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Eliminate the intermediate variables by using the data pronoun\n",
    "###\n",
    "\n",
    "# -- NO NEED TO EDIT; REWRITE THIS CODE -----\n",
    "# Set up a simple dataset\n",
    "df_initial = gr.df_make(\n",
    "    A=[1, 2, 3],\n",
    "    longcolumnname=[4, 5, 6],\n",
    ")\n",
    "print(df_initial)\n",
    "\n",
    "df_new = (\n",
    "    df_initial\n",
    "    >> gr.tf_rename(B=\"longcolumnname\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_new\n",
    "    >> gr.tf_filter(df_new.B == 5)\n",
    ")\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "(\n",
    "    df_initial\n",
    "    # Use pipes >> and the data pronoun DF;\n",
    "    # you should only need two lines of code\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3162e93",
   "metadata": {},
   "source": [
    "## Back to Verbs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ad735",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "We saw `tf_filter` above; this allows us to filter a dataset to only those rows satisfying some logical criterion. This makes answering basic questions about the data very easy. For instance, we might be interested in a particular processing method; we could find only those rows matching a specified method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(DF[\"PROPERTY: Processing method\"] == \"POWDER\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00d265",
   "metadata": {},
   "source": [
    "Notice that not all of the cells have useful values; some have `NaN` as their value (which means Not a Number). These could be due to any of a number of potential issues; perhaps the original reference did not report that value, meaning that information exists but is missing.\n",
    "\n",
    "There are some helper functions to help deal with `NaN` values in filters: `gr.not_nan(df.column)` will return `True` when its input is not `NaN`, while `gr.is_nan(df.column)` will do the reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a8a96",
   "metadata": {},
   "source": [
    "### __qX__ Filter the MPEA dataset to only those rows with a valid Yield Strength. Compare the original number of rows with the number of valid rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9447b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Filter the data to find the non-NaN Yield Strength values\n",
    "###\n",
    "\n",
    "# -- NO NEED TO EDIT; USE FOR COMPARISON -----\n",
    "print(\"Original shape: {}\".format(df_mpea.shape))\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0517d",
   "metadata": {},
   "source": [
    "### Mutating\n",
    "\n",
    "The `tf_mutate` verb allows us to create / modify columns based on existing column values. For instance, we could use a mutation to convert the units in a column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41133bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_mutate(\n",
    "        E_MPa = DF[\"PROPERTY: Young modulus (GPa)\"] * 1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380aa72",
   "metadata": {},
   "source": [
    "This might be useful if we aimed to compare two quantities; elasticity and ultimate tensile strength are somewhat related properties, so we might want to compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_mutate(\n",
    "        E_MPa = DF[\"PROPERTY: Young modulus (GPa)\"] * 1000\n",
    "    )\n",
    "    >> gr.tf_rename(\n",
    "        UTS_MPa = \"PROPERTY: UTS (MPa)\"\n",
    "    )\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF.UTS_MPa),\n",
    "        gr.not_nan(DF.E_MPa),\n",
    "    )\n",
    "    >> gr.tf_select(\n",
    "        \"UTS_MPa\",\n",
    "        \"E_MPa\",\n",
    "        gr.everything(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97d11c",
   "metadata": {},
   "source": [
    "### __QX__ Convert the weight parts per million (wppm) of Oxygen to a (weight) percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Convert wppm to a weight percentage\n",
    "# NOTE: There is some scaffolding code; you need only\n",
    "#       write the call to tf_mutate\n",
    "###\n",
    "\n",
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_filter(\n",
    "        gr.not_nan(DF[\"PROPERTY: O content (wppm)\"]),\n",
    "    )\n",
    "# -- WRITE YOUR CODE HERE -----\n",
    "    ## TODO: Use gr.tf_mutate to do the conversion\n",
    "\n",
    "    >> gr.tf_select(\"O_percent\", gr.everything())\n",
    "    >> gr.tf_head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba5bd",
   "metadata": {},
   "source": [
    "### Converting Data\n",
    "\n",
    "(TODO there's something wrong with the Nitrogen content column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_select(gr.contains(\"content\"))\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c35a35",
   "metadata": {},
   "source": [
    "The Oxygen and Carbon content columns are fine---they're `float64`, which is a numeric type as we'd expect. But the Nitrogen content is an `object`. Let's see what specific values this column takes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10def12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(\n",
    "    df_mpea[\"PROPERTY: N content (wppm)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804e151",
   "metadata": {},
   "source": [
    "It seems that the original data is mixed; some values are a numeric ppm value, while others are the qualitative statement `\"undetectable\"` (and yet others are simply missing). We can use the Grama helper `gr.as_numeric()` to help convert the data to a numeric type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(N_wppm=\"PROPERTY: N content (wppm)\")\n",
    "    >> gr.tf_mutate(N_converted = gr.as_numeric(DF.N_wppm))\n",
    "    >> gr.tf_select(\"N_wppm\", \"N_converted\")\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41465ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_mpea\n",
    "    >> gr.tf_rename(N_wppm=\"PROPERTY: N content (wppm)\")\n",
    "    >> gr.tf_mutate(N_converted = gr.as_numeric(DF.N_wppm))\n",
    "    >> gr.tf_select(\"N_wppm\", \"N_converted\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5b174",
   "metadata": {},
   "source": [
    "## Pivoting Data\n",
    "\n",
    "---\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3663aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.data import df_stang_wide\n",
    "df_stang_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97b49e",
   "metadata": {},
   "source": [
    "Our goal will be to wrangle this messy, wide dataset into tidy, long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8cdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grama.data import df_stang\n",
    "df_stang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10847148",
   "metadata": {},
   "source": [
    "(What does pivoting look like? Here's an example.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = (\n",
    "    gr.df_make(\n",
    "        A=[1, 2, 3],\n",
    "        B=[4, 5, 6],\n",
    "        C=[7, 8, 9],\n",
    "    )\n",
    ")\n",
    "print(df_tmp)\n",
    "\n",
    "(\n",
    "    df_tmp\n",
    "    >> gr.tf_pivot_longer(\n",
    "        columns=[\"A\", \"B\", \"C\"],\n",
    "        names_to=\"name\",\n",
    "        values_to=\"value\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b45f10",
   "metadata": {},
   "source": [
    "### __QX__ \n",
    "\n",
    "(Make sure to add an `observation` column with the `index_to` argument.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_qX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5bccb",
   "metadata": {},
   "source": [
    "Execute the following to check your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_qX.shape[0] == 54)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not sufficiently long; did you pivot?\")\n",
    "    \n",
    "try:\n",
    "    assert(df_qX.shape[1] == 5)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have five columns\")\n",
    "    \n",
    "try:\n",
    "    assert(\"observation\" in df_qX.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have five columns\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f17d1",
   "metadata": {},
   "source": [
    "### __QY__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcbc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qY = (\n",
    "    df_qX\n",
    "\n",
    ")\n",
    "df_qY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a0ea0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61446d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_qY.shape[0] == 54)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not the right length; how did that happen?\")\n",
    "    \n",
    "try:\n",
    "    assert(df_qY.shape[1] == 6)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have six columns\")\n",
    "    \n",
    "try:\n",
    "    assert(\"angle\" in df_qY.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'angle' column\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8e8b1",
   "metadata": {},
   "source": [
    "### __QZ__\n",
    "\n",
    "*Hint*: You should only need to set the `names_from` and `values_from` arguments with this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c70cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qZ = (\n",
    "    df_qY\n",
    "\n",
    ")\n",
    "df_qZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311305a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d87dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert(df_qZ.shape[0] == 27)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame is not the right length; how did that happen?\")\n",
    "    \n",
    "try:\n",
    "    assert(\"angle\" in df_qZ.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'angle' column\")\n",
    "    \n",
    "try:\n",
    "    assert(\"E\" in df_qZ.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'E' column\")\n",
    "    \n",
    "try:\n",
    "    assert(\"mu\" in df_qZ.columns)\n",
    "except AssertionError:\n",
    "    raise AssertionError(\"The DataFrame should have an 'mu' column\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f31a3f",
   "metadata": {},
   "source": [
    "### Bonus: One-step pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed22777",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_stang_wide\n",
    "    >> gr.tf_pivot_longer(\n",
    "        columns=[\"E_00\", \"mu_00\", \"E_45\", \"mu_45\", \"E_90\", \"mu_90\"],\n",
    "        names_to=[\".value\", \"angle\"],\n",
    "        names_sep=\"_\",\n",
    "        values_to=\"value\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32801369",
   "metadata": {},
   "source": [
    "## Wrangling Data\n",
    "[Hadley Wickham](http://hadley.nz/) -- author of the `tidyverse` and data science superstar -- notes that \"wrangling data is 80% boredom and 20% screaming\". To give you a sense of why this stuff is hard (but hopefully avoid the screaming), I'm leaving one of the wrangling steps in the workflow here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b73a7d",
   "metadata": {},
   "source": [
    "It's not obvious from the exercises above, but *there's an issue with these data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ba477",
   "metadata": {},
   "source": [
    "All of the entries are objects, not numbers! We'll need to convert these to numeric values. The following slightly-mysterious call will cast every column of `df_data` to a numeric type and modify the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7f7cc",
   "metadata": {},
   "source": [
    "Let's check the data types again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e1c43",
   "metadata": {},
   "source": [
    "These are numbers we can work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c38617",
   "metadata": {},
   "source": [
    "## Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569bb04",
   "metadata": {},
   "source": [
    "With the numerical issues above sorted out, we can carry out *quantitative* operations on the dataframe. One useful thing we can do is compute a set of *summaries* on the data using `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df294c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0142d17",
   "metadata": {},
   "source": [
    "These summaries include things like the `mean` and standard deviation (`std`), as well as quartiles of the data. These give us a sense of *typical* values; for instance, we can see that a large fraction of observations have a zero-\"Diffusion time\", but at least one observation has a value `> 70`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57027884",
   "metadata": {},
   "source": [
    "### Special indexing\n",
    "One of the most powerful features of pandas is the ability to do *logical indexing*; we may provide an array of `True` or `False` values to select only those rows with `True` values. For instance, we could do the following to select the third row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_boolean = [False] * df_data.shape[0]  # Mostly-false array\n",
    "idx_boolean[2] = True  # Make the third entry True\n",
    "df_data[idx_boolean]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834515b",
   "metadata": {},
   "source": [
    "Where this kind of *logical indexing* becomes helpful is when we chain this with the conditionals we learned in the previous exercise. For instance, we could use logic *using one of the columns* to effectively \"filter\" for variables that meet some condition. For instance, the following will filter for nonzero \"Carburization Time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[df_data[\"Carburization Time\"] > 0].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42969",
   "metadata": {},
   "source": [
    "### Q5: Basic data operations\n",
    "Once more, use the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) to learn how to do the following tasks:\n",
    "\n",
    "- Select only those rows for which \"Diffusion time\" is greater than 70\n",
    "- Sort df_data in descending order by \"Fatigue Strength\" and return the top 10\n",
    "- Take the average of \"Normalizing Temperature\" and \"Tempering Temperature\" and add the column \"avg_temp\" (You may need to Google how to do this one!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63740e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Basic data operations\n",
    "# TODO: Select rows for which \"Diffusion time\" > 70\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Basic data operations\n",
    "# TODO: Sort by \"Fatigue Strength\" in descending order, take the top-10\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Basic data operations\n",
    "# TODO: Average \"Normalizing Temperature\" and \"Tempering Temperature\" into the column \"avg_tmp\", return the head\n",
    "###\n",
    "\n",
    "# -- WRITE YOUR CODE BELOW -----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a6c24",
   "metadata": {},
   "source": [
    "## Endnotes\n",
    "\n",
    "- The data portions of Grama make heavy use of ideas from the [Tidyverse](https://www.tidyverse.org/); specifically the [dplyr](https://dplyr.tidyverse.org/) package. However, those packages are for the R programming language.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
